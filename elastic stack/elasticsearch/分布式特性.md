# Elasticsearch 分布式特性

    es集群由多个es实例组成，不同的集群通过集群名称进行区分
    [cerebo](https://github.com/lmenezes/cerebro/releases)

## Cluster State

    -节点信息：节点名称，链接地址等
    -索引信息：索引名称，配置

## Master Node

    可以修改master state的节点称为master节点，一个集群只能有一个master节点
    cluster state存储在每个节点上，master维护最新版本并同步给其他的节点
    master节点可以通过选举产生，可以被选举的节点称为master-eligible节点

## Coordinating Node

    处理请求的节点，该节点为所有节点的默认角色，不能取消
    作用：将所有的请求路由到正确的节点进行处理，eg:创建节点的请求转发到master节点进行处理

## 提高系统的可用性

    服务可用性：
        多节点情况下，允许其中的一些节点停止服务
    数据可用性
        引入副本
        每个节点上都保留有一份数据
    引入分片是数据可以分布在所有的节点上
    分片是es支持PB级别的数据的基石
    分片在创建索引是指定，并且不允许在更改
    分片有主副之分，泳衣实现数据的高可用
    主分片向副分片同步数据，可以有多个，从而提高读取的吞吐量

## 分片数

    过小会导致后续无法通过增加节点实现水平扩容
    过大会导致一个节点上的分片数量过多，造成资源的浪费，同时会影响到查询的性能

## Cluster Health

    green:主副分片都分配正常
    yellow:朱芬片都正常的分配，单身有副本分片为正常分配
    red:有朱芬片为未正常分配

## 故障转移 Failover

    当集群发生故障时，如何自动的进行修复
    当master节点无法响应一段时间后，会重新进行master节点的选举
    当新的master节点发现主分片下线了，集群变为red状态
    master节点发现主分片为分配，将另一个分片提升为新的主分片，集群变为yellow状态
    新的master节点会为生成新的副本，集群状态变为green

## 文档的分布式存储

    如何判断一个doc保存到哪个shard上？
        文档到分片的映射算法
        目的：使得文档可以均匀的分布在各个分片上，以充分的利用资源
        shard = hash(routing) % number_of_primary_shards
        保证将数据可以均匀的分布在各个分片上
        routing默认为文档的id,可以自行指定
        number_of_primary_shards：主分片数  
    文档批量创建的流程：
        coordinate（收到请求的节点）节点通过routing计算所有文档对应的shard,然后按照主shard分配执行对应的操作，同时将请求发送涉及的主shard
        主shard执行请求后将同样的请求发送的副本shard
        副本shard执行请求后将结果返回给主shard
        参与的主shard在将结果返回给coordinate节点
    文档批量读取的请求（mget请求）
        coordinate（收到请求的当前节点）通过routing计算得到文档对应的shard,然后以轮训的机制获取将要参与的shard,按照shard构造mget请求，同时将请求发送到对应的shard
        对应的shard获取结果并返回文档
        返回给client

## 脑裂问题

    通过设定参与选举的合法节点数量控制选举的进行，n/2+1

## shard详解

### 倒排索引的不可变更的特性

    -倒排索引一旦生成，就不能进行更改有如下好处：
        1、不用考虑并发写文件的问题，杜绝了锁机制带来的性能问题
        2、由于文件系统不在更改，可以充分的利用文件系统的缓存，只需要载入一次，之后都从内存中读取，性能好
        3、利于生成缓存数据
        4、利于对文件进行压缩存储
    坏处：
        当写入新的文档时，必须重新的构建倒索引，新的文档才能被检索到，导致文档的实时性较差
        当文件数量特别大的时候重新生成倒排索引文件的时长较长，导致新加入的文档不能及时的被索引到
    解决方案：
        新文档生成新的倒排索引文件，同事查询新来倒排索引文件，然后将结果进行汇总

### 文档搜索的实时性

#### segment

    Lucene采用了多倒排索引的方案，它将单个的倒排索引成为segment,和在一起成为Index,与ES中的Index不同，ES中的一个shard对应一个lucene的Index
    Lucene中会有个专门的文件记录所有的segment信息，称为commit point

#### refresh

    segment吸入磁盘的的过程依然非常的耗时，可以借助文件系统的缓存的特性，先将segment在缓存中创建并开放查询来进一步的提高实时性，该过程被称为refresh
    在refresh之前会把文档先存储在一个buffer中，refresh将buffer中的文档清空并生成一个segment
    es默认每一秒钟执行一次refresh

    发生的时机：
        时间间隔达到时，通过index.settings.refresh_interval设定，默认为1s
        index buffer沾满时，通过indices.memory.index_buffer_size设定，默认为jvm heap 的10%，所有shard共享
        flush时发生

#### translog

    问题：如果在segment还没有写入磁盘之前发生了宕机，可能导致文档无法恢复
    如何解决这个问题
        es引入了translog的机制，当有写文件到buffer中的时候，会同时将该操作写入translog
        translog文件会及时的写入（fsync），6之后的版本每个操作都会被计入translog,可以修改为多久写入一次
        es在启动的时候检查translog文件,并从中恢复数据

#### flush

    flush负责将内存中的数据写入磁盘
        1.translog
        2.清空index buffer ,生成segment
        3.将commit point写入磁盘
        4.执行fsync将内存中的segment写入磁盘
        5.删除旧的translog文件
    发生的时机
        1.间隔时间达到，默认30分钟，6.x之后无法修改
        2.translog沾满，可以通过index.translog.flush_threshold_size控制，默认512M每个index都有自己的translog

#### 删除与更新文档

    segment 一旦生成旧不能更改，那如何删除文档？
        Lucene专门维护了一个.del文件，记录所有已经删除的文档，.del记录的是文档自lucene内部的id,自查询结果之前会过滤掉所有的.del文件
    如何更新
        删除在创建新的文档

#### segment merge

    segment数量会不停的增多，查询速度会越来越慢
    es定时在后台会进行segment merge操作减少segment的数量
    或者通过froce_merge_api手动的强制进行segment的操作