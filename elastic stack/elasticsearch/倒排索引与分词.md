# 倒排索引与分词

## 书与搜索引擎

    书的目录对应正排索引
    索引对应倒排索引

### 正排索引

    从文档id到文档的内容，单词的关联关系

### 倒排索引

    搜索引擎的核心
    从单词到id的惯量关系

#### 倒排索引的组成

##### 单词词典

    记录所有文档的单词  ，一般都比较大
    记录单词到倒排列表的关联关系（记录单词关联了哪些文档）
    单词词典的一般实现方式为B+Tree

##### 倒排列表

    记录了单词到文档的集合，由倒排索引项（psoting）组成
    倒排索引项主要包含如下信息：
        文档id:用于获取原始的文档信息
        单词频率：记录单词在文档中出现的次数，用于相关性算分
        位置：记录单词在文档中的位置，用于进行词语搜索
        偏移：记录文档的开始和结束的位置，用于进行高亮显示
    es存储的是一个json格式的文档，其中包含多个字段，每个字段都会有自己的倒排索引

## 分词

    将文本转化为一系列单词的过程

### 分词器（Analyzer）

    是es中专门处理分词的组件
    1.Charater Filters
        针对原始文本进行处理，例如去除html特殊标记符
    2.Tokenizer
        将原始的文本按照一定规则切分为单词
    3.Token Filters
        针对Tokenizer处理的单词在进行加工，例如大小写转化，删除新增等处理

### Analyze API

    es提供的测试分词的api接口，方便分词效果，endpiont是_anaylze
    作用：
        1.指定analyzer进行测试
            # analyze
            POST _analyze
            {
                "analyzer": "standard",
                "text": ["hello world"]
            }
        2.指定索引中的字段进行测试
            POST panda/_analyze
            {
                "field": "username",
                "text": ["hello world"]
            }
        3.可以自定义分词器进行测试
            POST _analyze
            {
                "tokenizer": "standard",
                "filter": ["lowercase"],
                "text": ["Hello world"]
            }
        es自带的分词器
            1.standard:
                默认分词器
                组成
                    a.tokenizer:standard
                    b.token filters: standard,lowercase,stop(默认关闭，打开需要修改【配置)
                特性：
                    按词切分，支持多语言
                    小写处理
            2.simple
                按照非字母进行切分，小写处理
            3.wihtespace
                按照空格进行切分
            4.stop
                simple + stop word(去除一些语气性的修饰词，可以自定义)
            5.keyword
                不分词
            6.pattern
                通过正则表达式自定义分割符
                默认\w+,非字符的符号进行分割
            7.language
                一些常见语言的分词器

#### 中文分词

    将一个汉字的序列分成一个单独的词，英文中一空格作为分界，汉语中却没有一个形式上的分界符，上下文不同，可能导致分词的结果迥然不同（中文博大精深）
    常用的分词系统
        1.IK
            实现中文分词的切分，支持ik_smart,ik_maxword
            可以自定义词库，支持热更新分词词典\
        2.jieba
            python中流行的分词词典，支持分词和词性标注
            支持繁体分词，自定义词典，并行分词
    高阶分词系统：
    给予自然语言处理的分词系统
    -Hanlp:由一系列模型和算法组成的Java工具包，目标普及自然语言在生产环境中的应用
    -THULAC:由清华大学自然语言处理与社会人文计算实验室推出的一套中文词法分析工具包，具有中文分词和词性标注功能

#### 自定义分词器

    可以通过自定义Charater Filters ,Tokenizer 和 Token Filter 实现

##### Charater Filters

    在tokenizer之前对原始文本进行处理
    自带功能：
        HTML Strip:去除html标签和转换html实体
        mapping:进行字符替换操作
        pattern replace 进行正则匹配替换
    影响后续的解析操作（postion 和offset）
        POST _analyze
       {
            "tokenizer": "keyword",
            "char_filter": ["html_strip"],
           "text": ["<p>I&apos;m so <b>happy</b>!</p>"]
        }

##### Tokenizer

    将原始文本按照一定的规则切分成为单词
    自带如下：
        1.standard：按照单词进行分割
        2.letter:按照非字符类进行分割
        3.whitespace:按照空格进行分割
        4.UAX URL EMAIL：按照standard进行分割，但是不会分割url和email
        5.NGram和Edge NGram连词分割（输入提示）
        6.Path Hierarchy:按照文件路径进行分割

##### Token Filter

    对于tokenizer输出的单词进行增加，删除，修改等操作
    自带如下：
        1.lowercase:转小写
        2.stop:删除stop words
        3.NGram和Edge NGram连词分割
        4.Synonym:添加近义词的term

##### 自定义分词器api

    PUT panda
    {
        "settings": {
            "analysis": {
            "char_filter": {},
            "tokenizer": {},
            "filter": {},
            "analyzer": {}
            }
        }
    }

    eg:
        PUT test_index
        {
        "settings": {
            "analysis": {
            "filter": ["lowercase","asciifolding"],
            "analyzer": {
                "panda_analyzer":{
                "type":"custom",
                "tokenizer":"standard",
                "char_filter":["html_strip"]
                }
            }
            }
        }
        }

        POST test_index/_analyze
        {
        "analyzer": "panda_analyzer",
        "text": ["Is this <a>a  box </a>?"]
        }

##### 分词使用说明

    使用时机：
    创建或者更新文档的时候，对文档进行分词
        UT test_index
        {
            "mappings": {
                "doc":{
                    "properties":{
                        "title":{
                        "type":"text",
                        "analyzer":"whitespace"
                        }
                    }
                }
            }
        }
    查询时，对查询语句进行分词
    1.查询时使用
        POST test_index/_search
        {
            "query": {
                "match": {
                    "message": {
                        "query": "hello",
                        "analyzer": "standard"
                    }
                }
            }
        }
    2.创建索引时指定
        PUT test_index
        {
            "mappings": {
                "doc":{
                    "properties":{
                        "title":{
                        "type":"text",
                        "analyzer":"whitespace",
                        "search_analyzer":"standard"
                        }
                    }
                }
            }
        }
    3.分词使用建议
    明确是否需要分词，keyword可以节省空间和提高性能
    善于_analyze
    测试